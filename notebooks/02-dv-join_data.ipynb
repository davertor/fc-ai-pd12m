{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What this notebook does\n",
        "When downloading the dataset with `img2dataset` tool, the image width and height are not saved in the metadata. However, this information is saved in the parquet files. This notebook shows how to join the global metadata with the parquet files to get the width and height of the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "os.environ['AWS_CONFIG_FILE'] = '../.aws/config'\n",
        "\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import boto3\n",
        "import s3fs\n",
        "import polars as pl\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "from fc_ai_pd12m.utils import safe_write_ipc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure S3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def s3_auth():\n",
        "    session = boto3.session.Session(profile_name='default')\n",
        "    credentials = session.get_credentials().get_frozen_credentials()\n",
        "    \n",
        "    s3_fs = s3fs.S3FileSystem(\n",
        "        key=credentials.access_key,\n",
        "        secret=credentials.secret_key,\n",
        "        endpoint_url='https://s3.gra.io.cloud.ovh.net',\n",
        "    )\n",
        "    return s3_fs, credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "aws_region = 'gra'\n",
        "aws_endpoint_url = 'https://s3.gra.io.cloud.ovh.net'\n",
        "bucket_name = 'fc-gra-alejandria'\n",
        "ds_path = f'{bucket_name}/ds/public/PD12M'\n",
        "\n",
        "# Initialize boto3 S3 client\n",
        "s3_fs, credentials = s3_auth()\n",
        "s3_storage_options = {\n",
        "    \"aws_access_key_id\": credentials.access_key,\n",
        "    \"aws_secret_access_key\": credentials.secret_key,\n",
        "    \"endpoint_url\": aws_endpoint_url,\n",
        "    \"aws_region\": aws_region,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test different join methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df = pl.DataFrame(\n",
        "#     {\n",
        "#         \"foo\": [1, 2, 3],\n",
        "#         \"bar\": [6.0, 7.0, 8.0],\n",
        "#         \"ham\": [\"a\", \"b\", \"c\"],\n",
        "#     }\n",
        "# )\n",
        "# other_df = pl.DataFrame(\n",
        "#     {\n",
        "#         \"apple\": [\"x\", \"y\", \"z\"],\n",
        "#         \"ham\": [\"a\", \"b\", \"d\"],\n",
        "#     }\n",
        "# )\n",
        "\n",
        "# display(df)\n",
        "# display(other_df)\n",
        "\n",
        "# for strategy in [\"inner\", \"left\", \"right\", \"full\"]:\n",
        "#     print(f\"Strategy: {strategy}\")\n",
        "#     joined_df = df.join(other_df, on=\"ham\", how=strategy)\n",
        "#     display(joined_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orig_parquet_folder = Path(\"/ssd/datasets/pd12m/metadata\")\n",
        "if not orig_parquet_folder.exists():\n",
        "    raise FileNotFoundError(f\"Folder {orig_parquet_folder} does not exist\")\n",
        "\n",
        "orig_parquet_files = list(orig_parquet_folder.glob(\"*.parquet\"))\n",
        "orig_parquet_files = [str(file) for file in orig_parquet_files]\n",
        "if len(orig_parquet_files) == 0:\n",
        "    raise FileNotFoundError(f\"No parquet files found in {orig_parquet_folder}\")\n",
        "print(f\"Number of original parquet files: {len(orig_parquet_files)}\")\n",
        "\n",
        "downloaded_parquet_folder = \"s3://fc-gra-alejandria/ds/public/PD12M\"\n",
        "downloaded_parquet_files = list(s3_fs.glob(downloaded_parquet_folder + \"/*.parquet\"))\n",
        "downloaded_parquet_files = [f\"s3://{file}\" for file in downloaded_parquet_files]\n",
        "if len(downloaded_parquet_files) == 0:\n",
        "    raise FileNotFoundError(f\"No parquet files found in {downloaded_parquet_folder}\")\n",
        "print(f\"Number of downloaded parquet files: {len(downloaded_parquet_files)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def join_parquet_files_in_dataset(parquet_files: list[str], s3_storage_options: dict):\n",
        "    # Read parquet files in parallel using polars\n",
        "    dfs = []\n",
        "    for parquet_file in parquet_files:\n",
        "        if 's3://' in parquet_file:\n",
        "            df = pl.read_parquet(parquet_file, storage_options=s3_storage_options)\n",
        "        else:\n",
        "            df = pl.read_parquet(parquet_file)\n",
        "        dfs.append(df)\n",
        "    \n",
        "    # Concatenate all dataframes\n",
        "    final_df = pl.concat(dfs, how=\"vertical\")\n",
        "    \n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orig_df = join_parquet_files_in_dataset(\n",
        "    orig_parquet_files,\n",
        "    s3_storage_options=s3_storage_options\n",
        ")\n",
        "print(f\"Number of rows in original dataframe: {orig_df.height}\")\n",
        "\n",
        "downloaded_df = join_parquet_files_in_dataset(\n",
        "    downloaded_parquet_files,\n",
        "    s3_storage_options=s3_storage_options\n",
        ")\n",
        "print(f\"Number of rows in downloaded dataframe: {downloaded_df.height}\")\n",
        "\n",
        "# Join the dataframes on the `url` column\n",
        "joined_df = downloaded_df.join(orig_df, on=\"url\", how=\"left\")\n",
        "print(f\"Number of rows in joined dataframe: {joined_df.height}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Format the joined dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feather_path = Path(\"output/global_pd12m_data.feather\")\n",
        "if not feather_path.exists():\n",
        "    raise FileNotFoundError(f\"File {feather_path} does not exist\")\n",
        "\n",
        "df = pl.read_ipc(feather_path, storage_options=s3_storage_options)\n",
        "print(f\"Number of rows in joined dataframe: {joined_df.height}\")   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Drop columns that are not needed\n",
        "columns_to_drop = [\n",
        "    col for col in [\"error_message\", \"status\", \"width\", \"height\", \n",
        "                    \"original_width\", \"original_height\", \"sha256\", \n",
        "                    \"hash\", \"caption_right\", \"id\"] \n",
        "    if col in joined_df.columns\n",
        "]\n",
        "joined_df = joined_df.drop(columns_to_drop)\n",
        "\n",
        "# Rename columns\n",
        "columns_to_rename = {\n",
        "    \"width_right\": \"image_width\",\n",
        "    \"height_right\": \"image_height\",\n",
        "    \"key\": \"image_id\",\n",
        "}\n",
        "existing_keys = []\n",
        "for k in columns_to_rename.keys():\n",
        "    if k in joined_df.columns:\n",
        "        existing_keys.append(k)\n",
        "joined_df = joined_df.rename({k: v for k, v in columns_to_rename.items() if k in existing_keys})\n",
        "\n",
        "# Get image path\n",
        "joined_df = joined_df.with_columns(\n",
        "    pl.col(\"image_id\").str.slice(0, 5).cast(pl.Utf8).alias(\"parquet_id\"),\n",
        ")\n",
        "joined_df = joined_df.with_columns(\n",
        "    (pl.col(\"parquet_id\") + \"/\" + pl.col(\"image_id\") + \".jpg\").alias(\"image_path\"),\n",
        ")\n",
        "joined_df = joined_df.with_columns(\n",
        "    (pl.col(\"image_width\") / pl.col(\"image_height\")).cast(pl.Float64).alias(\"aspect_ratio\"),  \n",
        ")\n",
        "\n",
        "display(joined_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save the joined dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dest_path = \"output/global_pd12m_data.feather\"\n",
        "try:\n",
        "    safe_write_ipc(joined_df, dest_path, s3_fs=s3_fs)\n",
        "    print(f\"Joined dataframe saved to {dest_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error writing joined dataframe: {e}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
